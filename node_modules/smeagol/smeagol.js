// Smeagl, nodeJS crawler to get content in HTML pages


module.paths.unshift('/usr/local/lib/node_modules/');

var fs = require('fs'),
	jsdom = require('jsdom');

var settings,
	crawled,
	toCrawl,
	result;

exports.configure = function(config){
	settings = config,
	crawled = {},
	toCrawl = [],
	result = {};
}

exports.crawl = function(obj){

	if(settings.log == true){
		log = fs.createWriteStream('smeagol-log.txt', {'flags': 'a'});
		log.write(obj.uri+'\r\n');
	}

	jsdom.env(
	  obj.uri,
	  ['http://code.jquery.com/jquery.js'],
	  function (errors, window) {
	    
	    window.$('a').each(function(index){
	    	var url = window.$(this).attr('href');

	    	if(!toCrawl[url]){
	    		toCrawl.push(url);
	    	}
	    });

	    // console.log(toCrawl);

	    /* Verify url pattern */
		for(var i in settings['patterns']){
			var page_properties = settings['patterns'][i],
				re = new RegExp('^' + i + '$','gi');
			
			if(re.exec(obj.uri)){
				var contents = settings['patterns'][i]['find'];

				/* Get content in pattern.find */
				for(var find in contents){
					result[find] = eval('window.$'+contents[find]);
				}

			}
		}

	    /* Add to crawled urls */
	    crawled[obj.uri] = result

		if(typeof(settings.callback) == 'function'){
			settings.callback(result);
		}
		
		return result;
	  }
	);
}

